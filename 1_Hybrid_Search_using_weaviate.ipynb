{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SCCSMARTCODE/rag-mastery/blob/main/1_Hybrid_Search_using_weaviate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hQhagnrPLAq",
        "outputId": "f3e97360-83a0-4728-8b6c-933dcf4bf426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/323.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes==0.48.1\n",
        "!pip install accelerate transformers\n",
        "!pip install weaviate-client\n",
        "!pip install pipeline\n",
        "!pip install colorama\n",
        "%pip install -qU langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "import os\n",
        "import weaviate\n",
        "import textwrap\n",
        "import logging\n",
        "from colorama import Fore, Style, init\n",
        "from weaviate.classes.init import Auth\n",
        "from google.colab import userdata\n",
        "from weaviate.classes.config import Configure, DataType, Property, ReferenceProperty\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "dbfLuR2XbkPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "-3abSSbvKPRi"
      },
      "outputs": [],
      "source": [
        "model_path = \"microsoft/UserLM-8b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        "    ).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "LvRmT61vO2SL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea21af5-7d2a-4834-9885-f07131a94082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "\n",
        "    # generation control\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1,\n",
        "\n",
        "    # output control\n",
        "    max_new_tokens=2048,\n",
        "    num_return_sequences=1,\n",
        "\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "C1TNz4TYV4Fc"
      },
      "outputs": [],
      "source": [
        "# result = pipe(\"What is RAG\")\n",
        "# print(result)\n",
        "# print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"docs.pdf\"\n",
        "loader = PyPDFLoader(file_path)"
      ],
      "metadata": {
        "id": "5vfYUQ1DIRKO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "tSDvN4b9IXyC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs[0].model_dump_json()"
      ],
      "metadata": {
        "id": "Hxmkz43RIr7F"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best practice: store your credentials in environment variables\n",
        "weaviate_url = userdata.get('WEAVIATE_URL')\n",
        "weaviate_api_key = userdata.get('WEAVIATE_API_KEY')\n",
        "hf_api_key = userdata.get('HF_TOKEN')\n",
        "\n",
        "headers = {\n",
        "    \"X-HuggingFace-Api-Key\": hf_api_key,\n",
        "}\n",
        "\n",
        "# Connect to Weaviate Cloud\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=weaviate_url,\n",
        "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
        "    headers=headers\n",
        ")\n",
        "\n",
        "\n",
        "print(client.is_ready())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBoDRsxUxN_I",
        "outputId": "4a21df0d-dd8a-4a24-c240-f887610c6a2f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.collections.list_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3pZk06KB92S",
        "outputId": "1f78939a-7f40-4e61-f8b1-f340dcb159b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parent Collection ---\n",
        "client.collections.create(\n",
        "    name=\"Files\",\n",
        "    description=\"RAG Collection\",\n",
        "    vectorizer_config=Configure.Vectorizer.text2vec_huggingface(\n",
        "        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    ),\n",
        "    properties=[\n",
        "        Property(name=\"source\", data_type=DataType.TEXT),\n",
        "        Property(name=\"title\", data_type=DataType.TEXT),\n",
        "        Property(name=\"author\", data_type=DataType.TEXT),\n",
        "        Property(name=\"total_pages\", data_type=DataType.INT),\n",
        "    ],\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# --- Child Collection ---\n",
        "client.collections.create(\n",
        "    name=\"Pages\",\n",
        "    vectorizer_config=Configure.Vectorizer.text2vec_huggingface(\n",
        "        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    ),\n",
        "    properties=[\n",
        "        Property(name=\"content\", data_type=DataType.TEXT),\n",
        "        Property(name=\"page\", data_type=DataType.INT),\n",
        "    ],\n",
        "    references=[  # âœ… references go here, not in properties\n",
        "        ReferenceProperty(name=\"source_ref\", target_collection=\"Files\"),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgc2KCt95KsB",
        "outputId": "a73513bc-53d9-40fc-90a0-d79929e228c0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weaviate.collections.collection.sync.Collection at 0x7a2735c55910>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.collections.list_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxM4UBQxY74q",
        "outputId": "17ae7ae4-a17c-49e9-c59d-c77c41933875"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Files': _CollectionConfigSimple(name='Files', description='RAG Collection', generative_config=None, properties=[_Property(name='source', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None), _Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None), _Property(name='author', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None), _Property(name='total_pages', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None)], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2', 'useCache': True, 'useGPU': False, 'waitForModel': False}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None),\n",
              " 'Pages': _CollectionConfigSimple(name='Pages', description=None, generative_config=None, properties=[_Property(name='content', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None), _Property(name='page', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface', vectorizer_configs=None)], references=[_ReferenceProperty(name='source_ref', description=None, target_collections=['Files'])], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2', 'useCache': True, 'useGPU': False, 'waitForModel': False}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = client.collections.get(\"Files\")\n",
        "pages = client.collections.get(\"Pages\")"
      ],
      "metadata": {
        "id": "SjbD6QeWMZjn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_obj = files.data.insert(\n",
        "    properties={\n",
        "        \"source\": file_path,\n",
        "        \"title\": \"Retrieval-Augmented Generation\",\n",
        "        \"author\": \"Patrick Lewis et al.\",\n",
        "        \"total_pages\": 19\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "b2SzMZQHNldP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    pages.data.insert(\n",
        "        properties={\n",
        "            \"content\": doc.page_content,\n",
        "            \"page\": doc.metadata['page'],\n",
        "        },\n",
        "    references={\n",
        "        \"source_ref\": file_obj\n",
        "    }\n",
        "    )"
      ],
      "metadata": {
        "id": "fl7aAeQFOlQL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "init(autoreset=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\"\n",
        ")\n",
        "log = logging.getLogger(\"RAG-Pipeline\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEIi9rDMg4ME",
        "outputId": "a5fcdb57-bb52-4676-ee1f-1ac8fdadd92a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_hybrid(query: str, k: int = 5, alpha: float = 0.5):\n",
        "    \"\"\"Return list of (score, properties) from Weaviate hybrid query, with print debugging.\"\"\"\n",
        "    print(f\"{Fore.CYAN}\\nğŸ” Retrieving from Weaviate:\", query)\n",
        "    resp = pages.query.hybrid(query=query, alpha=alpha, limit=k)\n",
        "\n",
        "    hits = []\n",
        "    objs = getattr(resp, \"objects\", None) or []\n",
        "\n",
        "    if not objs:\n",
        "        print(f\"{Fore.YELLOW}âš ï¸  No results returned from Weaviate.\")\n",
        "        return hits\n",
        "\n",
        "    print(f\"{Fore.GREEN}âœ… Retrieved {len(objs)} candidates:\\n\")\n",
        "    for i, o in enumerate(objs, 1):\n",
        "        props = getattr(o, \"properties\", {}) or getattr(o, \"__dict__\", {}).get(\"properties\", {})\n",
        "        score = (\n",
        "            getattr(o, \"score\", None)\n",
        "            or getattr(o, \"certainty\", None)\n",
        "            or getattr(o, \"distance\", None)\n",
        "        )\n",
        "        hits.append((score, props))\n",
        "\n",
        "        snippet = (props.get(\"content\") or props.get(\"body\") or \"\")[:250].replace(\"\\n\", \" \")\n",
        "        title = props.get(\"title\") or props.get(\"source\") or \"Untitled\"\n",
        "        print(\n",
        "            f\"{Fore.LIGHTBLUE_EX}[{i}] {Style.BRIGHT}{title}{Style.RESET_ALL} \"\n",
        "            f\"| score={score} | page={props.get('page')}\\n\"\n",
        "            f\"    {snippet}...\"\n",
        "        )\n",
        "\n",
        "    return hits"
      ],
      "metadata": {
        "id": "CFNLdumNZw1P"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context(hits, max_chars=3000):\n",
        "    pieces = []\n",
        "    for i, (_, props) in enumerate(hits, 1):\n",
        "        title = props.get(\"title\") or props.get(\"source\") or \"\"\n",
        "        page = props.get(\"page\")\n",
        "        content = props.get(\"content\") or props.get(\"body\") or \"\"\n",
        "        header = f\"[{i}] {title} (page {page})\"\n",
        "        pieces.append(header + \"\\n\" + content.strip())\n",
        "    ctx = \"\\n\\n---\\n\\n\".join(pieces)\n",
        "    # truncate conservatively to avoid hitting token limits\n",
        "    return ctx if len(ctx) <= max_chars else ctx[:max_chars]"
      ],
      "metadata": {
        "id": "-kZLBNqphmO-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(question: str, k=5, alpha=0.5, max_new_tokens=2048):\n",
        "    print(f\"{Fore.MAGENTA}\\nğŸ§  Question:\", question)\n",
        "    hits = retrieve_hybrid(question, k=k, alpha=alpha)\n",
        "    if not hits:\n",
        "        print(f\"{Fore.RED}âŒ No relevant documents found.\")\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    context = build_context(hits)\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "        You are an assistant that answers using only the provided context below.\n",
        "        If the answer is not in the context, say \"I don't know\".\n",
        "        Cite sources using bracket numbers like [1], [2].\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer and cite sources.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    print(f\"{Fore.YELLOW}\\nğŸ“„ Constructed Prompt:\\n{'-'*80}\\n{prompt[:800]}...\\n{'-'*80}\")\n",
        "    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    generated = out[0][\"generated_text\"] if isinstance(out, list) else out[\"generated_text\"]\n",
        "\n",
        "    print(f\"{Fore.CYAN}\\nğŸ’¬ Model Output:\\n{'-'*80}\\n{generated}\\n{'-'*80}\")\n",
        "    return generated"
      ],
      "metadata": {
        "id": "X_7zo_jBhbIQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_answer(\"What is Retrieval-Augmented Generation (RAG)?\", k=4, alpha=0.5)\n",
        "print(\"\\nFinal Answer:\\n\", answer)"
      ],
      "metadata": {
        "id": "PcAv10K_etaf"
      },
      "execution_count": 82,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOTDNaASNQ/VaoYHQGSCO3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}